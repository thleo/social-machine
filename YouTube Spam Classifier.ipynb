{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Spam Youtube Comments\n",
    "### [Source](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)\n",
    "\n",
    "***\n",
    "#### Notes\n",
    "- Using comments from Psy video as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version check\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick note on `nltk.stopwords()`\n",
    "*for first time nltk users*\n",
    "\n",
    "In order to use this package, you have to install the `stopwords` package from the `nltk` download GUI.  \n",
    "This can be achieved by entering the following into python console:\n",
    "```python\n",
    ">>> import nltk\n",
    ">>> nltk.download()\n",
    "```\n",
    "\n",
    "Then the GUI will pop up, go to the corpus tab and find `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc</td>\n",
       "      <td>Jason Haddad</td>\n",
       "      <td>2013-11-26T02:55:11</td>\n",
       "      <td>Hey, check out my new website!! This site is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>z13lfzdo5vmdi1cm123te5uz2mqig1brz04</td>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>2013-11-27T21:39:24</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>z122wfnzgt30fhubn04cdn3xfx2mxzngsl40k</td>\n",
       "      <td>Bob Kanowski</td>\n",
       "      <td>2013-11-28T12:33:27</td>\n",
       "      <td>i turned it on mute as soon is i came on i jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>z13ttt1jcraqexk2o234ghbgzxymz1zzi04</td>\n",
       "      <td>Cony</td>\n",
       "      <td>2013-11-28T16:01:47</td>\n",
       "      <td>You should check my channel for Funny VIDEOS!!﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>z12avveb4xqiirsix04chxviiljryduwxg0</td>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28T16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "5  LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc      Jason Haddad   \n",
       "6          z13lfzdo5vmdi1cm123te5uz2mqig1brz04    ferleck ferles   \n",
       "7        z122wfnzgt30fhubn04cdn3xfx2mxzngsl40k      Bob Kanowski   \n",
       "8          z13ttt1jcraqexk2o234ghbgzxymz1zzi04              Cony   \n",
       "9          z12avveb4xqiirsix04chxviiljryduwxg0       BeBe Burkey   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "5  2013-11-26T02:55:11  Hey, check out my new website!! This site is a...   \n",
       "6  2013-11-27T21:39:24                          Subscribe to my channel ﻿   \n",
       "7  2013-11-28T12:33:27  i turned it on mute as soon is i came on i jus...   \n",
       "8  2013-11-28T16:01:47    You should check my channel for Funny VIDEOS!!﻿   \n",
       "9  2013-11-28T16:30:13  and u should.d check my channel and tell me wh...   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  \n",
       "5      1  \n",
       "6      1  \n",
       "7      0  \n",
       "8      1  \n",
       "9      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psy_data = 'data/Youtube01-Psy.csv'\n",
    "df_psy = pd.read_csv(psy_data)\n",
    "df_psy.head(10)\n",
    "# class: boolean for spam tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlist(slist):\n",
    "    l = []\n",
    "    for s in slist: # access each comment\n",
    "        x = re.sub(\"[^a-zA-Z]\",\" \", s) # replace punctuation with whitespace\n",
    "        l.append(x) # big list of cleaned comments\n",
    "    lower = [s.lower() for s in l] # still list of long strings\n",
    "    en_stopwords = set(stopwords.words(\"english\"))\n",
    "    words = [w for s in lower for w in s.split() if w not in en_stopwords]   \n",
    "    # returns a bag of words for all given string in list of strings\n",
    "    cleanstrings = []\n",
    "    return words\n",
    "    \n",
    "def cleanstrings(slist):\n",
    "    l = []\n",
    "    for s in slist: # access each comment\n",
    "        x = re.sub(\"[^a-zA-Z]\",\" \", s) # replace punctuation with whitespace\n",
    "        l.append(x) # big list of cleaned comments\n",
    "    lower = [s.lower() for s in l] # still list of long strings\n",
    "    en_stopwords = set(stopwords.words(\"english\"))\n",
    "    clean = []\n",
    "    for s in lower:\n",
    "        x = s\n",
    "        for w in s.split():\n",
    "            if w in en_stopwords:\n",
    "                x = x.replace(w,'',1)\n",
    "        clean.append(x.split())\n",
    "    return clean\n",
    "\n",
    "def clean_raw(s):\n",
    "    s = re.sub(\"[^a-zA-Z]\",\" \", s)\n",
    "    s = s.lower().split()\n",
    "    en_stopwords = set(stopwords.words(\"english\"))\n",
    "    x = [w for w in s if w not in en_stopwords]\n",
    "    return \" \".join(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those unfamiliar with list comprehension, I find this helpful:  \n",
    "[Understanding nested list comprehension syntax in Python](https://spapas.github.io/2016/04/27/python-nested-list-comprehensions/)\n",
    "\n",
    "the below code is equivalent of the above list comprehension:\n",
    "```python\n",
    "    words = []\n",
    "    for s in lower:\n",
    "        for w in s.split():\n",
    "            if w not in en_stopwords:\n",
    "                words.append(w)\n",
    "                \n",
    "                \n",
    "```            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 6, 4, 7, 8, 8, 8, 8, 3] \n",
      " {3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "# basic example of how lists work\n",
    "# lists are more efficient than sets when searching\n",
    "l1 = [5,5,5,6,4,7,8,8,8,8,3]\n",
    "s1 = set(l1)\n",
    "\n",
    "print(l1, '\\n', s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty words in comments (cum):   3141\n",
      "Unique words:  1237\n"
     ]
    }
   ],
   "source": [
    "com_list = list(df_psy['CONTENT'])\n",
    "clean_coms = cleanstrings(com_list) # list of lists of words in commments\n",
    "com_words = wordlist(com_list) # separates comments into list of strings of words\n",
    "\n",
    "print(\"Qty words in comments (cum):  \",len(com_words))\n",
    "unique_words = list(set(com_words))\n",
    "print(\"Unique words: \",len(unique_words))      \n",
    "# print(len(clean_coms), 'original qty comments:  ',len(com_list))\n",
    "\n",
    "clean_com_str = [clean_raw(s) for s in com_list] # this will be fed to vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['huh', 'anyway', 'check', 'tube', 'channel', 'kobyoshi'], ['hey', 'guys', 'check', 'new', 'chnnel', 'frst', 'vid', 'us', 'onkeys', 'i', 'm', 'monkey', 'white', 'shirt', 'please', 'leave', 'a', 'like', 'comment', 'please', 'subscribe'], ['test', 'say', 'murdev', 'com']]\n",
      "['huh', 'anyway', 'check', 'tube', 'channel', 'kobyoshi', 'hey', 'guys', 'check', 'new']\n",
      "['grateful', 'stupid', 'listening', 'shit', 'firo', 'round', 'dont', 'shitty', 'free', 'friend']\n",
      "['huh anyway check tube channel kobyoshi', 'hey guys check new channel first vid us monkeys monkey white shirt please leave like comment please subscribe']\n"
     ]
    }
   ],
   "source": [
    "# to see current items, run below\n",
    "print(clean_coms[:3])\n",
    "print(com_words[:10])\n",
    "print(unique_words[:10]) # contains unique words only\n",
    "print(clean_com_str[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                         tokenizer=None, #default\n",
    "                         preprocessor=None, #default\n",
    "                         stop_words=None, #default\n",
    "                         max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = vectorizer.fit_transform(clean_com_str)\n",
    "train_features = train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
